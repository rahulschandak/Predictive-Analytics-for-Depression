# -*- coding: utf-8 -*-
"""project_idmp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15i85Z6gN1yKly0eDdgnHKz4qqCOER9ln

Importing libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

"""Importing and cleaning dataset"""

df = pd.read_csv("StreamlitData.csv")
df.info()
pulse_mean = df['Pulse'].mean()
df['Pulse'] = df['Pulse'].replace(0, pulse_mean)
BMI_mean = df['BMI'].mean()
df['BMI'] = df['BMI'].replace(0, BMI_mean)
sleephour_mean = df['Sleep Hours'].mean()
df['Sleep Hours'] = df['Sleep Hours'].replace(0, sleephour_mean)

print(df.isnull().sum())

"""Selecting all categorical variables"""

df.columns
string_cols = df.select_dtypes(include=['object']).columns.tolist()
print(string_cols)

"""Converting categorical to numerical data"""

le=LabelEncoder()
for col in string_cols:
   df[col] = le.fit_transform(df[col])

"""Top 30 variables based on correlation coefficients"""

corr_dict = {}
# Compute the correlation coefficient for each column
for col in df.columns:
    corr_coef = df['Depression'].corr(df[col])
    # Store the coefficient in the dictionary with the column name as key
    corr_dict[col] = corr_coef
# Sort the dictionary by the correlation coefficient in descending order
sorted_dict = {k: v for k, v in sorted(corr_dict.items(), key=lambda item: abs(item[1]),reverse=True)}
# Get the list of top 30 column names with highest correlation
top30_cols = list(sorted_dict.keys())[:30]

print(top30_cols)

""" Top 30 variables based on P Value"""

pVal_dict = {}
for col in df.columns:
  corr,p_val=pearsonr(df['Depression'],df[col])
  pVal_dict[col] = p_val
sorted_dict2 = {k: v for k, v in sorted(pVal_dict.items(), key=lambda item: abs(item[1]), reverse=False)}
top30_cols2 = list(sorted_dict2.keys())[:30]

print(top30_cols)   # Top 30 according to Correlation Coefficient
print(top30_cols2)  # Top 30 according to P Value

"""Feature Selection"""

mychoice_columns = ['Depression', 'Gender', 'Age', 'Race','Marital Status','Pregnant','Household Income','Ever Overweight','Pulse','Sleep Hours','BMI']

# Feature Selection
selected_dataset = df[list(set(mychoice_columns + top30_cols))]
selected_dataset

"""Correlation matrix for the filtered dataset"""

df_corr=selected_dataset.corr()
print(df_corr)

"""Analyzing summary statistics"""

summary_stats = selected_dataset.describe()
print(summary_stats)

selected_dataset.boxplot(column=['Current Cigarettes Per Day'])

corr_matrix = selected_dataset.corr()
sns.heatmap(corr_matrix,cmap="YlGnBu")

selected_dataset['Depression'].value_counts()

depressed=selected_dataset[selected_dataset['Depression']==0]
notdepressed=selected_dataset[selected_dataset['Depression']==1]

selected_dataset.dtypes

selected_dataset.columns

feature=selected_dataset[['Health Problem Back Or Neck', 'Health Problem Weight',
       'Current Cigarettes Per Day', 'Rx Gabapentin', 'Pulse',
       'Memory Problems', 'Gender', 'Bronchitis Currently', 'Out Of Work',
       'Heart Attack Relative', 'Health Problem Bone Or Joint',
       'Rx Clonazepam', 'Cant Work', 'Age', 'Race', 'Arthritis',
       'Walking Equipment', 'Health Problem Arthritis', 'Limited Work',
       'Arthritis Type', 'Health Problem Diabetes', 'Vigorous Recreation',
       'Health Problem Breathing', 'Trouble Sleeping History', 'Rx Alprazolam',
       'Household Income', 'Marital Status', 'Pregnant',
       'Healthcare Equipment', 'BMI', 'Prescriptions Count', 'Ever Overweight',
       'Asthma Currently', 'Health Problem Vision', 'Sleep Hours',
       'Bronchitis', 'Moderate Recreation', 'Health Problem Blood Pressure']]

x=np.asarray(feature)
y=np.asarray(selected_dataset['Depression'])
x.shape

"""Splitting dataset into training and test sets"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

"""SVM Model """

from sklearn import svm
from sklearn.metrics import roc_curve, confusion_matrix, auc, classification_report
sv=svm.SVC(kernel='linear',gamma="auto", C = 0.3)

sv.fit(x_train,y_train)

# Evaluate the model's performance on the test set
y_pred=sv.predict(x_test)
print(classification_report(y_test,y_pred))

# # Generate the confusion matrix
# cm = confusion_matrix(y_test, y_pred)
# print("Confusion Matrix:")
# print(cm)

# # Plot the confusion matrix
# plt.imshow(cm, cmap='Blues', interpolation='nearest')
# plt.title('Confusion Matrix')
# plt.colorbar()
# tick_marks = np.arange(len(set(y_test)))
# plt.xticks(tick_marks, sorted(list(set(y_test))))
# plt.yticks(tick_marks, sorted(list(set(y_test))))
# plt.xlabel('Predicted')
# plt.ylabel('True')
# plt.show()

# Get predicted probabilities for test set
probabilities = sv.decision_function(x_test)

# Get false positive rate, true positive rate, and threshold values for ROC curve
fpr, tpr, thresholds = roc_curve(y_test, probabilities)

# Calculate AUC score
auc_score = auc(fpr, tpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve : SVM')
plt.legend(loc="lower right")
plt.show()

# Get predicted labels for test set
pred_labels = sv.predict(x_test)

# Get confusion matrix
cm = confusion_matrix(y_test, pred_labels)

# Plot confusion matrix
sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)
plt.title("Confusion Matrix : SVM")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
train_data, test_data, train_labels, test_labels = train_test_split(x,y,test_size=0.2)

rf = RandomForestClassifier()
rf.fit(train_data, train_labels)

# Evaluate the model's performance on the test set
accuracy = rf.score(test_data, test_labels)
print("Accuracy:", accuracy)

# predictions=rf.predict(test_data)
# cm = confusion_matrix(test_labels, predictions)
# print("Confusion Matrix:")
# print(cm)

# Get predicted labels for test set
pred_labels = rf.predict(test_data)

# Get confusion matrix
cm = confusion_matrix(test_labels, pred_labels)

# Plot confusion matrix
sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)
plt.title("Confusion Matrix: Random Forest")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Get predicted probabilities for test set
probabilities = rf.predict_proba(test_data)[:, 1]

# Get false positive rate, true positive rate, and threshold values for ROC curve
fpr, tpr, thresholds = roc_curve(test_labels, probabilities)

# Calculate AUC score
auc_score = auc(fpr, tpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve : Random Forest')
plt.legend(loc="lower right")
plt.show()

"""Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Train model
clf=GradientBoostingClassifier()
clf.fit(train_data,train_labels)

# Evaluate the model's performance on the test set
y_pred=clf.predict(test_data)
accuracy=accuracy_score(test_labels,y_pred)
print('Accuracy:',clf.score(test_data, test_labels))

# predictions=clf.predict(test_data)
# cm = confusion_matrix(test_labels, predictions)
# print("Confusion Matrix:")
# print(cm)

from sklearn.metrics import roc_curve, confusion_matrix, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Get predicted probabilities for test set
probabilities = clf.predict_proba(test_data)[:, 1]

# Get false positive rate, true positive rate, and threshold values for ROC curve
fpr, tpr, thresholds = roc_curve(test_labels, probabilities)

# Calculate AUC score
auc_score = auc(fpr, tpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.title("ROC curve:Gradient Boosting Classifier")
plt.show()

# Get predicted labels for test set
pred_labels = clf.predict(test_data)

# Get confusion matrix
cm = confusion_matrix(test_labels, pred_labels)

# Plot confusion matrix
sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)

plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion matrix : Gradient Boosting Classifier")
plt.show()

"""Gaussian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_curve, average_precision_score

nb = GaussianNB()
nb.fit(train_data, train_labels)

accuracy = nb.score(test_data, test_labels)
print("Accuracy:", accuracy)

predictions=nb.predict(test_data)
cm = confusion_matrix(test_labels, predictions)
print("Confusion Matrix:")
print(cm)

# Plot the confusion matrix
sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)

y_score = nb.predict_proba(x_test)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

# Compute precision-recall curve and AUPRC
precision, recall, thresholds = precision_recall_curve(y_test, y_score)
average_precision = average_precision_score(y_test, y_score)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_data, train_labels)

accuracy = lr.score(test_data, test_labels)
print("Accuracy:", accuracy)

"""ROC Curves for all models"""

from sklearn.metrics import roc_curve, roc_auc_score

# Define the models to evaluate
models = [rf, clf, nb, lr, sv]

# Plot the ROC curves for each model in a single graph
plt.figure(figsize=(8, 6))
for model in models:
    # Check if the model has predict_proba method
    if hasattr(model, 'predict_proba'):
        # Predict the probabilities for the positive class
        y_prob = model.predict_proba(test_data)[:, 1]
    else:
        # Use decision function if predict_proba is not available
        y_prob = model.decision_function(test_data)
        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())
    # Compute the false positive rate and true positive rate
    fpr, tpr, _ = roc_curve(test_labels, y_prob)
    # Compute the AUC score
    auc_score = roc_auc_score(test_labels, y_prob)
    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{type(model).__name__}')
    
# Set the plot title and axis labels
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# Show the legend
plt.legend()
# Show the plot
plt.show()